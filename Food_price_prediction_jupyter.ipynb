{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e5d60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf188cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (0.18.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (4.62.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.7.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.3.4)\n",
      "Requirement already satisfied: deap>=1.2 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.3.1)\n",
      "Requirement already satisfied: xgboost>=1.1.0 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tpot) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->tpot) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->tpot) (0.4.4)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\cmpl2\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Install tpot on the server\n",
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4d8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1434206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/cmpl2/Documents/corn_price_test.csv\n"
     ]
    }
   ],
   "source": [
    "#food_price_test = pd.read_csv(\"C:/Users/REVANTH/Desktop/ISB AMPBA/Term -6/FP2/corn_price_test.csv\")\n",
    "food_price_test =  \"C:/Users/cmpl2/Documents/corn_price_test.csv\"\n",
    "print(food_price_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00e28400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/cmpl2/Documents/cleaned_corn_price.csv\n"
     ]
    }
   ],
   "source": [
    "#cleaned_food_price = pd.read_csv(\"C:/Users/REVANTH/Desktop/ISB AMPBA/Term -6/FP2/Dataset Cleaned files/cleaned_corn_price.csv\")\n",
    "cleaned_food_price = \"C:/Users/cmpl2/Documents/cleaned_corn_price.csv\"\n",
    "print(cleaned_food_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c43eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.360e+02 2.016e+03 3.000e+00 3.100e+01]\n",
      " [3.520e+02 2.016e+03 3.000e+00 3.000e+01]\n",
      " [3.560e+02 2.016e+03 3.000e+00 2.900e+01]\n",
      " [3.540e+02 2.016e+03 3.000e+00 2.800e+01]\n",
      " [3.530e+02 2.016e+03 3.000e+00 2.400e+01]\n",
      " [3.520e+02 2.016e+03 3.000e+00 2.300e+01]\n",
      " [3.520e+02 2.016e+03 3.000e+00 2.200e+01]\n",
      " [3.520e+02 2.016e+03 3.000e+00 2.100e+01]\n",
      " [3.490e+02 2.016e+03 3.000e+00 1.800e+01]\n",
      " [3.500e+02 2.016e+03 3.000e+00 1.700e+01]\n",
      " [3.500e+02 2.016e+03 3.000e+00 1.600e+01]\n",
      " [3.500e+02 2.016e+03 3.000e+00 1.500e+01]\n",
      " [3.500e+02 2.016e+03 3.000e+00 1.400e+01]\n",
      " [3.460e+02 2.016e+03 3.000e+00 1.100e+01]\n",
      " [3.440e+02 2.016e+03 3.000e+00 1.000e+01]\n",
      " [3.420e+02 2.016e+03 3.000e+00 9.000e+00]\n",
      " [3.420e+02 2.016e+03 3.000e+00 8.000e+00]\n",
      " [3.410e+02 2.016e+03 3.000e+00 7.000e+00]\n",
      " [3.410e+02 2.016e+03 3.000e+00 4.000e+00]\n",
      " [3.410e+02 2.016e+03 3.000e+00 3.000e+00]\n",
      " [3.410e+02 2.016e+03 3.000e+00 2.000e+00]\n",
      " [3.430e+02 2.016e+03 3.000e+00 1.000e+00]\n",
      " [3.440e+02 2.016e+03 2.000e+00 2.900e+01]\n",
      " [3.450e+02 2.016e+03 2.000e+00 2.600e+01]\n",
      " [3.470e+02 2.016e+03 2.000e+00 2.500e+01]\n",
      " [3.510e+02 2.016e+03 2.000e+00 2.400e+01]\n",
      " [3.550e+02 2.016e+03 2.000e+00 2.300e+01]\n",
      " [3.610e+02 2.016e+03 2.000e+00 2.200e+01]\n",
      " [3.580e+02 2.016e+03 2.000e+00 1.900e+01]\n",
      " [3.570e+02 2.016e+03 2.000e+00 1.800e+01]\n",
      " [3.580e+02 2.016e+03 2.000e+00 1.700e+01]\n",
      " [3.530e+02 2.016e+03 2.000e+00 1.600e+01]\n",
      " [3.520e+02 2.016e+03 2.000e+00 1.200e+01]\n",
      " [3.540e+02 2.016e+03 2.000e+00 1.100e+01]\n",
      " [3.540e+02 2.016e+03 2.000e+00 1.000e+01]\n",
      " [3.560e+02 2.016e+03 2.000e+00 9.000e+00]\n",
      " [3.560e+02 2.016e+03 2.000e+00 8.000e+00]\n",
      " [3.600e+02 2.016e+03 2.000e+00 5.000e+00]\n",
      " [3.640e+02 2.016e+03 2.000e+00 4.000e+00]\n",
      " [3.670e+02 2.016e+03 2.000e+00 3.000e+00]\n",
      " [3.670e+02 2.016e+03 2.000e+00 2.000e+00]\n",
      " [3.670e+02 2.016e+03 2.000e+00 1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# open files\n",
    "test = np.genfromtxt(food_price_test,delimiter=',',skip_header=1)\n",
    "train = np.genfromtxt(cleaned_food_price,delimiter=',',skip_header=1)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b52f6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of training without class\n",
    "train_new = np.delete(train,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84044de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of class variables\n",
    "train_class = train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2dadd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data inti training and testing sets\n",
    "x_train,x_test, y_train ,y_test = train_test_split(train_new, train_class, train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ba16989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate tpot instance\n",
    "tpot = TPOTRegressor(verbosity=3, generations=10,scoring='balanced_accuracy', population_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "635ccb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/275 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5306487493987494\tAdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(27, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5306487493987494\tAdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5306487493987494\tAdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(27, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 27, n_neighbors = 47.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5346285473785475\tAdaBoostRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.15000000000000002), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 27, n_neighbors = 39.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 27, n_neighbors = 71.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5346285473785475\tAdaBoostRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.9, GradientBoostingRegressor__learning_rate=0.1, GradientBoostingRegressor__loss=huber, GradientBoostingRegressor__max_depth=2, GradientBoostingRegressor__max_features=0.25, GradientBoostingRegressor__min_samples_leaf=5, GradientBoostingRegressor__min_samples_split=16, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.15000000000000002), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5481746031746032\tAdaBoostRegressor(PCA(input_matrix, PCA__iterated_power=10, PCA__svd_solver=randomized), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5481746031746032\tAdaBoostRegressor(PCA(input_matrix, PCA__iterated_power=10, PCA__svd_solver=randomized), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5481746031746032\tAdaBoostRegressor(PCA(input_matrix, PCA__iterated_power=10, PCA__svd_solver=randomized), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5534920634920636\tAdaBoostRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.1, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.5290729918229918\tAdaBoostRegressor(input_matrix, AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-2\t0.5534920634920636\tAdaBoostRegressor(SelectFromModel(input_matrix, SelectFromModel__ExtraTreesRegressor__max_features=0.1, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n",
      "\n",
      "-3\t0.5690981240981241\tAdaBoostRegressor(AdaBoostRegressor(RobustScaler(input_matrix), AdaBoostRegressor__learning_rate=0.1, AdaBoostRegressor__loss=linear, AdaBoostRegressor__n_estimators=100), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTRegressor(generations=10, population_size=25, scoring='balanced_accuracy',\n",
       "              verbosity=3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call fit function\n",
    "tpot.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9665063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPOT score: 0.5025252525252526\n"
     ]
    }
   ],
   "source": [
    "#call the score function on cv data\n",
    "print('TPOT score: {}'.format(tpot.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5000e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict prices for each month for next 5 years\n",
    "submission = tpot.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ac10827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of results for each month/years\n",
    "final = pd.DataFrame ({'year': test[:,0],'month':test[:,1],'Pred':submission})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdda9afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:\n",
      "19.65659657802515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE:')\n",
    "print(mean_squared_error(y_test, tpot.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec334fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\n",
      "4.433576048521684\n"
     ]
    }
   ],
   "source": [
    "print('RMSE:')\n",
    "print(np.sqrt(mean_squared_error(y_test, tpot.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8d3b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export pipeline\n",
    "export_filename = 'Food Price Prediction Pipeline.py'\n",
    "tpot.export(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5ee0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export predicted values\n",
    "final_filename = 'C:/Users/cmpl2/Documents/corn_price_pred.csv'\n",
    "final.to_csv(final_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ee198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
